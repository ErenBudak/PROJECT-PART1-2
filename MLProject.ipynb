{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be812c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.model_selection import GridSearchCV, TimeSeriesSplit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec4f884",
   "metadata": {},
   "source": [
    "RANDOM FOREST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "314b4b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SklearnEarthquakePredictor(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, n_estimators=23, max_depth=12, min_samples_split=7, \n",
    "                 shiftnum=3, max_features=None, random_state=None):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.shiftnum = shiftnum\n",
    "        self.max_features = max_features\n",
    "        self.random_state = random_state\n",
    "        self.target_names = ['futuremag', 'futuredepth', 'futurelat', 'futurelon']#tahmin yapacağımız kısımlar\n",
    "        self.rf_model = RandomForestRegressor(\n",
    "            n_estimators=self.n_estimators,\n",
    "            max_depth=self.max_depth,\n",
    "            min_samples_split=self.min_samples_split,\n",
    "            max_features=self.max_features,\n",
    "            random_state=self.random_state,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "    \n",
    "    def prepare(self, df):\n",
    "        dfshifts = df.copy()\n",
    "        for i in range(1, self.shiftnum + 1):\n",
    "            dfshifts[f'mag{i}'] = df['mag'].shift(i)\n",
    "            dfshifts[f'depth{i}'] = df['depth'].shift(i)\n",
    "            dfshifts[f'lat{i}'] = df['latitude'].shift(i)\n",
    "            dfshifts[f'lon{i}'] = df['longitude'].shift(i)\n",
    "        feature_cols = [f'{v}{i}' for i in range(1, self.shiftnum + 1) \n",
    "                       for v in ('mag', 'depth', 'lat', 'lon')]\n",
    "        target_cols = ['futuremag', 'futuredepth', 'futurelat', 'futurelon']\n",
    "        available_feature_cols = [col for col in feature_cols if col in dfshifts.columns]#bu kontrolü eklemek yapay zeka yardımıyla yaptığım bir şeydi\n",
    "        available_target_cols = [col for col in target_cols if col in dfshifts.columns]#öncesinde hepsinde sadece feature_cols ve target_cols yazıyordu. \n",
    "        X = dfshifts[available_feature_cols]\n",
    "        Y = dfshifts[available_target_cols]\n",
    "        combined = pd.concat([X, Y], axis=1)\n",
    "        combined = combined.dropna()\n",
    "        Xnew = combined[available_feature_cols].values\n",
    "        Ynew = combined[available_target_cols].values\n",
    "        return Xnew, Ynew\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        df = X\n",
    "        X_processed, Y_processed = self.prepare(df)\n",
    "        self.rf_model.fit(X_processed, Y_processed)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, df):\n",
    "        X_processed, _ = self.prepare(df)\n",
    "        predictions = self.rf_model.predict(X_processed)  \n",
    "        return predictions\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79503d23",
   "metadata": {},
   "source": [
    "XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d976dcde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "class XGBoostEarthquakePredictor(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, n_estimators=100, max_depth=6, learning_rate=0.1, \n",
    "                 subsample=0.8, colsample_bytree=0.8, min_child_weight=1,\n",
    "                 reg_alpha=0, reg_lambda=1, shiftnum=3, random_state=None):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.learning_rate = learning_rate\n",
    "        self.subsample = subsample\n",
    "        self.colsample_bytree = colsample_bytree\n",
    "        self.min_child_weight = min_child_weight\n",
    "        self.reg_alpha = reg_alpha\n",
    "        self.reg_lambda = reg_lambda\n",
    "        self.shiftnum = shiftnum\n",
    "        self.random_state = random_state\n",
    "        self.target_names = ['futuremag', 'futuredepth', 'futurelat', 'futurelon']\n",
    "        \n",
    "        \n",
    "        self.fitted_feature_columns = None\n",
    "        \n",
    "        self.xgb_model = XGBRegressor(\n",
    "            n_estimators=self.n_estimators,\n",
    "            max_depth=self.max_depth,\n",
    "            learning_rate=self.learning_rate,\n",
    "            subsample=self.subsample,\n",
    "            colsample_bytree=self.colsample_bytree,\n",
    "            min_child_weight=self.min_child_weight,\n",
    "            reg_alpha=self.reg_alpha,\n",
    "            reg_lambda=self.reg_lambda,\n",
    "            random_state=self.random_state,\n",
    "            n_jobs=-1,\n",
    "            verbosity=0\n",
    "        )\n",
    "    \n",
    "    def _preprocess_data(self, df, is_training=False):\n",
    "        df_processed = df.copy()\n",
    "        columns_to_drop = ['id', 'place', 'updated', 'locationSource']\n",
    "        columns_to_drop = [col for col in columns_to_drop if col in df_processed.columns]\n",
    "        if columns_to_drop:\n",
    "            df_processed = df_processed.drop(columns=columns_to_drop)\n",
    "        categorical_cols = ['magType', 'type', 'status', 'net']\n",
    "        available_categorical_cols = [col for col in categorical_cols if col in df_processed.columns]\n",
    "        \n",
    "        if available_categorical_cols:\n",
    "            # One-hot encoding için https://www.datacamp.com/tutorial/category/ai den yazı okudum,kodlamada yer yer yapay zekadan yardım aldım\n",
    "            df_processed = pd.get_dummies(df_processed, columns=available_categorical_cols, \n",
    "                                        prefix=available_categorical_cols, dummy_na=False)\n",
    "        \n",
    "        return df_processed\n",
    "    \n",
    "    def prepare(self, df, is_training=False):\n",
    "        df_processed = self._preprocess_data(df, is_training)\n",
    "        \n",
    "        dfshifts = df_processed.copy()\n",
    "        \n",
    "        for i in range(1, self.shiftnum + 1):\n",
    "            dfshifts[f'mag{i}'] = df_processed['mag'].shift(i)\n",
    "            dfshifts[f'depth{i}'] = df_processed['depth'].shift(i)\n",
    "            dfshifts[f'lat{i}'] = df_processed['latitude'].shift(i)\n",
    "            dfshifts[f'lon{i}'] = df_processed['longitude'].shift(i)\n",
    "        \n",
    "        \n",
    "        base_feature_cols = [f'{v}{i}' for i in range(1, self.shiftnum + 1) \n",
    "                           for v in ('mag', 'depth', 'lat', 'lon')]\n",
    "        \n",
    "        \n",
    "        target_cols = ['futuremag', 'futuredepth', 'futurelat', 'futurelon']\n",
    "        #Prepare metodunun  in buradan sonrasında yapay zeka X ve Y yi oluşturduğumuz yere kadarki 10 satırda yardım etti \n",
    "    \n",
    "        available_base_features = [col for col in base_feature_cols if col in dfshifts.columns]\n",
    "        available_target_cols = [col for col in target_cols if col in dfshifts.columns]\n",
    "        \n",
    "       \n",
    "        categorical_feature_cols = [col for col in dfshifts.columns \n",
    "                                   if any(cat in col for cat in ['magType_', 'type_', 'status_', 'net_'])]\n",
    "        \n",
    "       \n",
    "        if is_training:\n",
    "            self.fitted_feature_columns = available_base_features + categorical_feature_cols\n",
    "            all_feature_cols = self.fitted_feature_columns\n",
    "        else:\n",
    "            for col in self.fitted_feature_columns:\n",
    "                if col not in dfshifts.columns:\n",
    "                    dfshifts[col] = 0\n",
    "                    \n",
    "            all_feature_cols = self.fitted_feature_columns\n",
    "        \n",
    "        X = dfshifts[all_feature_cols]\n",
    "        Y = dfshifts[available_target_cols]\n",
    "        \n",
    "       \n",
    "        combined = pd.concat([X, Y], axis=1)\n",
    "        combined = combined.dropna()\n",
    "        \n",
    "\n",
    "        Xnew = combined[all_feature_cols].values\n",
    "        Ynew = combined[available_target_cols].values\n",
    "        \n",
    "        \n",
    "        \n",
    "        return Xnew, Ynew\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        df = X\n",
    "        X_processed, Y_processed = self.prepare(df, is_training=True)\n",
    "        self.xgb_model.fit(X_processed, Y_processed)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, df):\n",
    "        X_processed, _ = self.prepare(df, is_training=False)\n",
    "        predictions = self.xgb_model.predict(X_processed)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64d6812",
   "metadata": {},
   "source": [
    "MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456360b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "class MLPEarthquakePredictor(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, hidden_layer_sizes=(100,), activation='relu', solver='adam',\n",
    "                 alpha=0.0001, learning_rate_init=0.001, max_iter=1000,\n",
    "                 shiftnum=3, random_state=None):\n",
    "        self.hidden_layer_sizes = hidden_layer_sizes\n",
    "        self.activation = activation\n",
    "        self.solver = solver\n",
    "        self.alpha = alpha\n",
    "        self.learning_rate_init = learning_rate_init\n",
    "        self.max_iter = max_iter\n",
    "        self.shiftnum = shiftnum\n",
    "        self.random_state = random_state\n",
    "        self.target_names = ['futuremag', 'futuredepth', 'futurelat', 'futurelon']\n",
    "        \n",
    "        # Scaler ve imputer'ları saklamak için\n",
    "        self.feature_scaler = None\n",
    "        self.target_scaler = None\n",
    "        self.feature_imputer = None\n",
    "        self.target_imputer = None\n",
    "        self.fitted_feature_columns = None\n",
    "        \n",
    "        self.mlp_model = MLPRegressor(\n",
    "            hidden_layer_sizes=self.hidden_layer_sizes,\n",
    "            activation=self.activation,\n",
    "            solver=self.solver,\n",
    "            alpha=self.alpha,\n",
    "            learning_rate_init=self.learning_rate_init,\n",
    "            max_iter=self.max_iter,\n",
    "            random_state=self.random_state\n",
    "        )\n",
    "    \n",
    "    def _preprocess_data(self, df, is_training=False):\n",
    "        df_processed = df.copy()\n",
    "        \n",
    "        # Gereksiz sütunları kaldır\n",
    "        columns_to_drop = ['id', 'place', 'updated', 'locationSource']\n",
    "        columns_to_drop = [col for col in columns_to_drop if col in df_processed.columns]\n",
    "        if columns_to_drop:\n",
    "            df_processed = df_processed.drop(columns=columns_to_drop)\n",
    "        \n",
    "        # Kategorik sütunlar için one-hot encoding\n",
    "        categorical_cols = ['magType', 'type', 'status', 'net']\n",
    "        available_categorical_cols = [col for col in categorical_cols if col in df_processed.columns]\n",
    "        \n",
    "        if available_categorical_cols:\n",
    "            df_processed = pd.get_dummies(df_processed, columns=available_categorical_cols, \n",
    "                                        prefix=available_categorical_cols, dummy_na=False)\n",
    "        \n",
    "        return df_processed\n",
    "    \n",
    "    def prepare(self, df, is_training=False):\n",
    "        df_processed = self._preprocess_data(df, is_training)\n",
    "        dfshifts = df_processed.copy()\n",
    "        for i in range(1, self.shiftnum + 1):\n",
    "            dfshifts[f'mag{i}'] = df_processed['mag'].shift(i)\n",
    "            dfshifts[f'depth{i}'] = df_processed['depth'].shift(i)\n",
    "            dfshifts[f'lat{i}'] = df_processed['latitude'].shift(i)\n",
    "            dfshifts[f'lon{i}'] = df_processed['longitude'].shift(i)\n",
    "        \n",
    "        base_feature_cols = [f'{v}{i}' for i in range(1, self.shiftnum + 1) \n",
    "                           for v in ('mag', 'depth', 'lat', 'lon')]\n",
    "        \n",
    "        target_cols = ['futuremag', 'futuredepth', 'futurelat', 'futurelon']\n",
    "        \n",
    "        available_base_features = [col for col in base_feature_cols if col in dfshifts.columns]\n",
    "        available_target_cols = [col for col in target_cols if col in dfshifts.columns]\n",
    "        \n",
    "        categorical_feature_cols = [col for col in dfshifts.columns \n",
    "                                   if any(cat in col for cat in ['magType_', 'type_', 'status_', 'net_'])]\n",
    "        \n",
    "        if is_training:\n",
    "            self.fitted_feature_columns = available_base_features + categorical_feature_cols\n",
    "            all_feature_cols = self.fitted_feature_columns\n",
    "        else:\n",
    "            for col in self.fitted_feature_columns:\n",
    "                if col not in dfshifts.columns:\n",
    "                    dfshifts[col] = 0\n",
    "                    \n",
    "            all_feature_cols = self.fitted_feature_columns\n",
    "        \n",
    "        X = dfshifts[all_feature_cols]\n",
    "        Y = dfshifts[available_target_cols]\n",
    "        \n",
    "        \n",
    "        combined = pd.concat([X, Y], axis=1)\n",
    "        combined = combined.dropna()\n",
    "        \n",
    "        Xnew = combined[all_feature_cols]\n",
    "        Ynew = combined[available_target_cols]\n",
    "        \n",
    "        \n",
    "        if is_training:\n",
    "            #\n",
    "            self.feature_imputer = SimpleImputer(strategy='mean')\n",
    "            self.feature_scaler = MinMaxScaler()\n",
    "            \n",
    "            # Target imputer ve scaler\n",
    "            self.target_imputer = SimpleImputer(strategy='mean')\n",
    "            self.target_scaler = MinMaxScaler()\n",
    "            \n",
    "            # Fit ve transform\n",
    "            Xnew_imputed = self.feature_imputer.fit_transform(Xnew)\n",
    "            Xnew_scaled = self.feature_scaler.fit_transform(Xnew_imputed)\n",
    "            \n",
    "            Ynew_imputed = self.target_imputer.fit_transform(Ynew)\n",
    "            Ynew_scaled = self.target_scaler.fit_transform(Ynew_imputed)\n",
    "            \n",
    "        else:\n",
    "            # Transform only\n",
    "            Xnew_imputed = self.feature_imputer.transform(Xnew)\n",
    "            Xnew_scaled = self.feature_scaler.transform(Xnew_imputed)\n",
    "            \n",
    "            Ynew_imputed = self.target_imputer.transform(Ynew)\n",
    "            Ynew_scaled = self.target_scaler.transform(Ynew_imputed)\n",
    "        \n",
    "        return Xnew_scaled, Ynew_scaled\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        df = X\n",
    "        X_processed, Y_processed = self.prepare(df, is_training=True)\n",
    "        self.mlp_model.fit(X_processed, Y_processed)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, df):\n",
    "        X_processed, _ = self.prepare(df, is_training=False)\n",
    "        predictions_scaled = self.mlp_model.predict(X_processed)\n",
    "        predictions = self.target_scaler.inverse_transform(predictions_scaled)#0-1 aralığından normale dönüş\n",
    "        return predictions\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
