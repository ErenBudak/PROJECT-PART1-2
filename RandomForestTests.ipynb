{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70f9e0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import GridSearchCV, TimeSeriesSplit\n",
    "%run MLProject.ipynb\n",
    "\n",
    "df=pd.read_csv('usgs_main.csv')\n",
    "df1=df\n",
    "df1['time'] = pd.to_datetime(df1['time'])\n",
    "df1 = df1.sort_values('time')#KRONOLOJİK\n",
    "df1 = df1.dropna(subset=['latitude','longitude','depth','mag','time'])#EKSİK DEĞERLERİ ATIŞ\n",
    "df1['lats'] = np.floor(df1['latitude']).astype(int)\n",
    "df1['lons'] = np.floor(df1['longitude']).astype(int)\n",
    "df1.set_index('time')\n",
    "dfweek = df1.set_index('time').resample('W').apply({\n",
    "    'mag':'mean',\n",
    "    'latitude':'mean',\n",
    "    'longitude':'mean',\n",
    "    'depth':'mean',\n",
    "})\n",
    "dfweek = dfweek.reset_index(drop=True)\n",
    "dfweek.index = dfweek.index + 1\n",
    "dfweek.index.name = 'timeindex'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad6d150",
   "metadata": {},
   "source": [
    "Verileri zamana göre sıraladık,koordinatlarını,derinliğini,büyüklüğünü ve zamanını elde tuttuk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c481215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 3125 candidates, totalling 9375 fits\n",
      "   Magnitude   : MSE=0.036, MAE=0.164, R²=-2.731\n",
      "   Depth       : MSE=11.905, MAE=2.709, R²=-0.970\n",
      "   Latitude    : MSE=1.788, MAE=1.086, R²=-0.160\n",
      "   Longitude   : MSE=15.720, MAE=3.022, R²=0.144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\eren1\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1108: UserWarning: One or more of the test scores are non-finite: [nan nan nan ... nan nan nan]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "dfweek['futuremag']=dfweek['mag'].shift(-1)\n",
    "dfweek['futuredepth']=dfweek['depth'].shift(-1)\n",
    "dfweek['futurelat']=dfweek['latitude'].shift(-1)\n",
    "dfweek['futurelon']=dfweek['longitude'].shift(-1)\n",
    "dfweek=dfweek.dropna(subset=['futuremag','futuredepth','futurelat','futurelon'])#eksikleri atalım\n",
    "Y=dfweek[['futuremag','futuredepth','futurelat','futurelon']]\n",
    "Y=Y.dropna().reset_index(drop=True)\n",
    "hiperparametreler = {\n",
    "    'n_estimators': [50, 100, 150,200,250],\n",
    "    'max_depth': [4, 8, 10,12,13],\n",
    "    'shiftnum': [2, 3, 4,5,6],\n",
    "    'min_samples_split': [10, 20, 23,24,28],\n",
    "    'max_features': [3, 5,6,7,8]\n",
    "}\n",
    "train = dfweek[:int((4*len(dfweek))/5)]\n",
    "test = dfweek[int(4*len(dfweek)/5):]\n",
    "splitter=TimeSeriesSplit(n_splits=3) # altta gridsearch ile ilgili yazı linki var,burada verbose ve n_jobs değişkenlerine koymam gerekenleri öğrendim\n",
    "grid = GridSearchCV( #https://www.mygreatlearning.com/blog/gridsearchcv.\n",
    "    estimator=SklearnEarthquakePredictor(random_state=42),\n",
    "    param_grid=hiperparametreler,\n",
    "    cv=splitter,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    n_jobs=-1, #-1 ken bütün işlemciler kullanılıyormuş\n",
    "    verbose=1  #aşamaları yazdırır \n",
    ")\n",
    "grid.fit(train)\n",
    "best_model = grid.best_estimator_\n",
    "test_predictions = best_model.predict(test)\n",
    "test_actual = test[['futuremag', 'futuredepth', 'futurelat', 'futurelon']].dropna()\n",
    "min_len = min(len(test_predictions), len(test_actual))\n",
    "test_mse = mean_squared_error(test_actual.iloc[:min_len].values, test_predictions[:min_len])\n",
    "target_names = ['futuremag', 'futuredepth', 'futurelat', 'futurelon']\n",
    "target_labels = ['Magnitude', 'Depth', 'Latitude', 'Longitude']\n",
    "#Tüm işlemler yapılınca MAE MSE R2 gibi metriklerin döngüsünü yapay zekaya yazdırdım\n",
    "for i, (name, label) in enumerate(zip(target_names, target_labels)):\n",
    "    if i < test_predictions.shape[1] and i < test_actual.shape[1]:\n",
    "        target_mse = mean_squared_error(\n",
    "            test_actual.iloc[:min_len, i], \n",
    "            test_predictions[:min_len, i]\n",
    "        )\n",
    "        target_mae = mean_absolute_error(\n",
    "            test_actual.iloc[:min_len, i], \n",
    "            test_predictions[:min_len, i]\n",
    "        )\n",
    "        target_r2 = r2_score(\n",
    "            test_actual.iloc[:min_len, i], \n",
    "            test_predictions[:min_len, i]\n",
    "        )\n",
    "        \n",
    "        print(f\"   {label:12}: MSE={target_mse:.3f}, MAE={target_mae:.3f}, R²={target_r2:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f7096d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 4, 'max_features': 3, 'min_samples_split': 10, 'n_estimators': 50, 'shiftnum': 2}\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import json\n",
    "\n",
    "joblib.dump(best_model, 'models/random_forest_dataset1.pkl')#kaydetmemde yapay zeka yardımcı oldu\n",
    "rf_best_params = {\n",
    "    'dataset1_params': grid.best_params_\n",
    "}\n",
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef99f8c6",
   "metadata": {},
   "source": [
    "RANDOM FOREST-DATASET2\n",
    "Çok benzer,sadece gerekli ufak değişiklikler yapıldı,hiperparametre değerleri değiştirildi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31c2f652",
   "metadata": {},
   "outputs": [],
   "source": [
    "pn=pd.read_csv('Significant Earthquake Dataset 1900-2023.csv')\n",
    "df2=pn\n",
    "df2=df2.rename(columns={'Time':'time','Mag':'mag','Depth':'depth','Latitude':'latitude','Longitude':'longitude' })#küçük harf uyuşmazlığı olmasın diye\n",
    "df2['time'] = pd.to_datetime(df2['time'])\n",
    "df2 = df2.sort_values('time')\n",
    "df2 = df2.dropna(subset=['latitude','longitude','depth','mag','time'])\n",
    "df2['lats'] = np.floor(df2['latitude']).astype(int)\n",
    "df2['lons'] = np.floor(df2['longitude']).astype(int)\n",
    "dfyear = df2.set_index('time').resample('YE').apply({\n",
    "    'mag':'mean',\n",
    "    'latitude':'mean',\n",
    "    'longitude':'mean',\n",
    "    'depth':'mean',\n",
    "})\n",
    "dfyear = dfyear.reset_index(drop=True)\n",
    "dfyear.index = dfyear.index + 1\n",
    "dfyear.index.name = 'timeindex'\n",
    "dfyear['futuremag']=dfyear['mag'].shift(-1)\n",
    "dfyear['futuredepth']=dfyear['depth'].shift(-1)\n",
    "dfyear['futurelat']=dfyear['latitude'].shift(-1)\n",
    "dfyear['futurelon']=dfyear['longitude'].shift(-1)\n",
    "dfyear=dfyear.dropna(subset=['futuremag','futuredepth','futurelat','futurelon'])\n",
    "Y_2=dfyear[['futuremag','futuredepth','futurelat','futurelon']]\n",
    "Y_2=Y_2.dropna().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91d2d7dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 3125 candidates, totalling 9375 fits\n",
      "   Magnitude   : MSE=0.001, MAE=0.017, R²=-0.821\n",
      "   Depth       : MSE=205.904, MAE=12.962, R²=-0.741\n",
      "   Latitude    : MSE=15.185, MAE=3.078, R²=-0.271\n",
      "   Longitude   : MSE=221.888, MAE=12.331, R²=-0.415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\eren1\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1108: UserWarning: One or more of the test scores are non-finite: [nan nan nan ... nan nan nan]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "hiperparametreler_2 = {\n",
    "    'n_estimators': [100,200,300,400,500],\n",
    "    'max_depth': [4, 9, 11,14,15],\n",
    "    'shiftnum': [4, 5, 6,7,8],\n",
    "    'min_samples_split': [10, 20,30,35,45],\n",
    "    'max_features': [3, 5,6,8,10]\n",
    "}\n",
    "train_2 = dfyear[:int((4*len(dfyear))/5)]\n",
    "test_2 = dfyear[int(4*len(dfyear)/5):]\n",
    "splitter_2=TimeSeriesSplit(n_splits=3)\n",
    "grid_2 = GridSearchCV(\n",
    "    estimator=SklearnEarthquakePredictor(random_state=42),\n",
    "    param_grid=hiperparametreler_2,\n",
    "    cv=splitter_2,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "grid_2.fit(train_2)\n",
    "best_model_2 = grid_2.best_estimator_\n",
    "test_predictions_2 = best_model_2.predict(test_2)\n",
    "test_actual_2 = test_2[['futuremag', 'futuredepth', 'futurelat', 'futurelon']].dropna()\n",
    "min_len = min(len(test_predictions_2), len(test_actual_2))\n",
    "test_mse_2 = mean_squared_error(test_actual_2.iloc[:min_len].values, test_predictions_2[:min_len])\n",
    "target_names = ['futuremag', 'futuredepth', 'futurelat', 'futurelon']\n",
    "target_labels = ['Magnitude', 'Depth', 'Latitude', 'Longitude']\n",
    "for i, (name, label) in enumerate(zip(target_names, target_labels)):\n",
    "    if i < test_predictions_2.shape[1] and i < test_actual_2.shape[1]:\n",
    "        target_mse = mean_squared_error(\n",
    "            test_actual_2.iloc[:min_len, i], \n",
    "            test_predictions_2[:min_len, i]\n",
    "        )\n",
    "        target_mae = mean_absolute_error(\n",
    "            test_actual_2.iloc[:min_len, i], \n",
    "            test_predictions_2[:min_len, i]\n",
    "        )\n",
    "        target_r2 = r2_score(\n",
    "            test_actual_2.iloc[:min_len, i], \n",
    "            test_predictions_2[:min_len, i]\n",
    "        )\n",
    "        \n",
    "        print(f\"   {label:12}: MSE={target_mse:.3f}, MAE={target_mae:.3f}, R²={target_r2:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b011ce60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 4, 'max_features': 3, 'min_samples_split': 10, 'n_estimators': 100, 'shiftnum': 4}\n"
     ]
    }
   ],
   "source": [
    "joblib.dump(best_model_2, 'models/random_forest_dataset2.pkl')\n",
    "rf_best_params['dataset2_params'] = grid_2.best_params_\n",
    "print(grid_2.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777a6bb6",
   "metadata": {},
   "source": [
    "RANDOM FOREST EXTRA-TİME STEP SAYISINI ARTTIRIP HİPERPARAMETRE DEĞERLERİNİ KÜÇÜLTEREK DAHA İYİ BİR SONUÇ ARAYACAĞIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ec079a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "hiper=pd.read_csv('Significant Earthquake Dataset 1900-2023.csv')\n",
    "rfext=hiper\n",
    "rfext=rfext.rename(columns={'Time':'time','Mag':'mag','Depth':'depth','Latitude':'latitude','Longitude':'longitude' })\n",
    "rfext['time'] = pd.to_datetime(rfext['time'])\n",
    "rfext = rfext.sort_values('time')\n",
    "rfext = rfext.dropna(subset=['latitude','longitude','depth','mag','time'])\n",
    "rfext['lats'] = np.floor(rfext['latitude']).astype(int)\n",
    "rfext['lons'] = np.floor(rfext['longitude']).astype(int)\n",
    "dfplus = rfext.set_index('time').resample('W').apply({\n",
    "    'mag':'mean',\n",
    "    'latitude':'mean',\n",
    "    'longitude':'mean',\n",
    "    'depth':'mean',\n",
    "})\n",
    "dfplus = dfplus.reset_index(drop=True)\n",
    "dfplus.index = dfplus.index + 1\n",
    "dfplus.index.name = 'timeindex'\n",
    "dfplus['futuremag']=dfplus['mag'].shift(-1)\n",
    "dfplus['futuredepth']=dfplus['depth'].shift(-1)\n",
    "dfplus['futurelat']=dfplus['latitude'].shift(-1)\n",
    "dfplus['futurelon']=dfplus['longitude'].shift(-1)\n",
    "dfplus=dfplus.dropna(subset=['futuremag','futuredepth','futurelat','futurelon'])\n",
    "Y_2=dfplus[['futuremag','futuredepth','futurelat','futurelon']]\n",
    "Y_2=Y_2.dropna().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca03387b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 3125 candidates, totalling 9375 fits\n",
      "   Magnitude   : MSE=0.025, MAE=0.126, R²=-0.227\n",
      "   Depth       : MSE=2208.496, MAE=35.196, R²=0.112\n",
      "   Latitude    : MSE=217.520, MAE=11.554, R²=-0.203\n",
      "   Longitude   : MSE=2896.299, MAE=43.082, R²=0.014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\eren1\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1108: UserWarning: One or more of the test scores are non-finite: [nan nan nan ... nan nan nan]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "hiperparametreler_yen = {\n",
    "    'n_estimators': [50,60,70,80,90],\n",
    "    'max_depth': [3, 5, 8,11,13],\n",
    "    'shiftnum': [2, 3, 5,7,9],\n",
    "    'min_samples_split': [15, 18,22,32,40],\n",
    "    'max_features': [4,5,6,7,8]\n",
    "}\n",
    "train_art = dfplus[:int((4*len(dfplus))/5)]\n",
    "test_art = dfplus[int(4*len(dfplus)/5):]\n",
    "splitter_art=TimeSeriesSplit(n_splits=3)\n",
    "grid_max = GridSearchCV(\n",
    "    estimator=SklearnEarthquakePredictor(random_state=42),\n",
    "    param_grid=hiperparametreler_yen,\n",
    "    cv=splitter_art,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "grid_max.fit(train_art)\n",
    "best_model_plus = grid_max.best_estimator_\n",
    "test_predictions_ex = best_model_plus.predict(test_art)\n",
    "test_actual_ex = test_art[['futuremag', 'futuredepth', 'futurelat', 'futurelon']].dropna()\n",
    "min_len = min(len(test_predictions_ex), len(test_actual_ex))\n",
    "target_names = ['futuremag', 'futuredepth', 'futurelat', 'futurelon']\n",
    "target_labels = ['Magnitude', 'Depth', 'Latitude', 'Longitude']\n",
    "for i, (name, label) in enumerate(zip(target_names, target_labels)):\n",
    "    if i < test_predictions_ex.shape[1] and i < test_actual_ex.shape[1]:\n",
    "        target_mse = mean_squared_error(\n",
    "            test_actual_ex.iloc[:min_len, i], \n",
    "            test_predictions_ex[:min_len, i]\n",
    "        )\n",
    "        target_mae = mean_absolute_error(\n",
    "            test_actual_ex.iloc[:min_len, i], \n",
    "            test_predictions_ex[:min_len, i]\n",
    "        )\n",
    "        target_r2 = r2_score(\n",
    "            test_actual_ex.iloc[:min_len, i], \n",
    "            test_predictions_ex[:min_len, i]\n",
    "        )\n",
    "        \n",
    "        print(f\"   {label:12}: MSE={target_mse:.3f}, MAE={target_mae:.3f}, R²={target_r2:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d4545551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 3, 'max_features': 4, 'min_samples_split': 15, 'n_estimators': 50, 'shiftnum': 2}\n"
     ]
    }
   ],
   "source": [
    "joblib.dump(best_model_plus, 'models/random_forest_extra.pkl')\n",
    "rf_best_params['extra_params'] = grid_max.best_params_\n",
    "print(grid_max.best_params_)\n",
    "with open('models/random_forest_best_params.json', 'w') as f:\n",
    "    json.dump(rf_best_params, f, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
